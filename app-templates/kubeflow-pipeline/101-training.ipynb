{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 101 Github Issue Summarization\n",
    "\n",
    "\n",
    "In this notebook, we will show how to:\n",
    "\n",
    "* Define a seq2seq model\n",
    "* Perform training with [Keras](https://keras.io) and [Tensorflow](https://www.tensorflow.org/api_docs/python/tf)\n",
    "* Validate model [Seldon](https://docs.seldon.io/projects/seldon-core/en/latest/python/api/modules.html)\n",
    "\n",
    "To perform the training we have wired few technologies together. \n",
    "* **S3 bucket** as mounted a file system [here](../bucket): this is place for all training artifacts. Keras prefers to work with the artifacts as it would be normal files.\n",
    "* **Notebook profile** [here](profile_default/startup) contains meaningful notebook defaults and settings\n",
    "* **SuperHub integration** contains various environment provisioning scripts \n",
    "* **Git integration** your notebook has a git repository. It is wise to make periodical commits into the git. Best way to do it is to use Jupyter terminal\n",
    "\n",
    "In the beginning of the scrip we define a variables that will be later used for data preprocess and training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext extensions\n",
    "\n",
    "import os\n",
    "\n",
    "rubbish = 'latest'\n",
    "\n",
    "ARTIFACTS_ROOT = f\"{os.environ['HOME']}/bucket/training-{rubbish}\"\n",
    "DATASET_FILE = f\"{ARTIFACTS_ROOT}/dataset.csv\"\n",
    "MODEL_FILE = f\"{ARTIFACTS_ROOT}/training1.h5\"\n",
    "TITLE_PP_FILE = f\"{ARTIFACTS_ROOT}/title_preprocessor.dpkl\"\n",
    "BODY_PP_FILE = f\"{ARTIFACTS_ROOT}/body_preprocessor.dpkl\"\n",
    "TRAIN_DF_FILE = f\"{ARTIFACTS_ROOT}/traindf.csv\"\n",
    "TEST_DF_FILE =  f\"{ARTIFACTS_ROOT}/testdf.csv\"\n",
    "TRAIN_TITLE_VECS = f\"{ARTIFACTS_ROOT}/train_title_vecs.npy\"\n",
    "TRAIN_BODY_VECS = f\"{ARTIFACTS_ROOT}/train_body_vecs.npy\"\n",
    "\n",
    "TRAINING_DATA_SIZE = 2000\n",
    "TEST_SIZE = .10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training we need to download a dataset file in a CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# github issues small: 2Mi data set (best for dev/test)\n",
    "SAMPLE_DATASET = 'https://s3.us-east-2.amazonaws.com/asi-kubeflow-models/gh-issues/data-sample.csv'\n",
    "SAMPLE_DATASET_MD5 = '916af946f2fe1d1779b26205d4d8378f'\n",
    "# data set for 3Gi. (best for training)\n",
    "FULL_DATASET = 'https://s3.us-east-2.amazonaws.com/asi-kubeflow-models/gh-issues/data-full.csv'\n",
    "FULL_DATASET_MD5 = '57dc987c04d41a94d0d9daf4d0ebf8ba'\n",
    "\n",
    "from extensions.utils import download_file\n",
    "\n",
    "%time\n",
    "os.makedirs(ARTIFACTS_ROOT, exist_ok=True)\n",
    "download_file(\n",
    "    url=SAMPLE_DATASET, \n",
    "    md5checksum=SAMPLE_DATASET_MD5, \n",
    "    download_to=DATASET_FILE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data\n",
    "\n",
    "Before we process data for machine learning, we need to split data into training and test data sets (variable `TEST_SIZE`). To accelerate a training we can also limit data size (variable `TRAINING_DATA_SIZE`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "if TRAINING_DATA_SIZE:\n",
    "    traindf, testdf = train_test_split(pd.read_csv(DATASET_FILE).sample(n=TRAINING_DATA_SIZE), test_size=TEST_SIZE)\n",
    "else:\n",
    "    traindf, testdf = train_test_split(pd.read_csv(DATASET_FILE),test_size=TEST_SIZE)\n",
    "\n",
    "print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns')\n",
    "print(f'Test: {testdf.shape[0]:,} rows {testdf.shape[1]:,} columns')\n",
    "\n",
    "# preview data\n",
    "traindf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data for Machine Learning\n",
    "Here we will use `ktext` library [documentation](https://github.com/hamelsmu/ktext)\n",
    "\n",
    "We will convert text into a vector and do the same for title and body:\n",
    "\n",
    "* **body**: Clean, tokenize, and apply padding / truncating such that each document `length = 70` also, retain only the top `8,000` words in the vocabulary and set the remaining words to 1 which will become common index for rare words\n",
    "\n",
    "* **title**: Instantiate a text processor for the titles, with some different parameters `append_indicators=True` appends the tokens `_start_` and `_end_` to each document. `padding='post'` means that zero padding is appended to the end of the of the document (as opposed to the default which is 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from ktext.preprocess import processor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "train_body_raw = traindf.body.tolist()\n",
    "body_pp = processor(keep_n=8000, padding_maxlen=70)\n",
    "train_body_vecs = body_pp.fit_transform(train_body_raw)\n",
    "\n",
    "train_title_raw = traindf.issue_title.tolist()\n",
    "title_pp = processor(append_indicators=True, keep_n=4500, padding_maxlen=12, padding ='post')\n",
    "train_title_vecs = title_pp.fit_transform(train_title_raw)\n",
    "\n",
    "# preview\n",
    "data = np.array([['Before', train_title_raw[0], train_body_raw[0]],\n",
    "                ['After', train_title_vecs[0], train_body_vecs[0]]])\n",
    "df = pd.DataFrame(data=data, columns=['', 'Issue Title', 'Issue body'])\n",
    "display(HTML(df.to_html(index=False)))\n",
    "\n",
    "# Save the preprocessor\n",
    "print(f\"Saving {BODY_PP_FILE}\")\n",
    "with open(BODY_PP_FILE, 'wb') as f:\n",
    "    dpickle.dump(body_pp, f)\n",
    "\n",
    "print(f\"Saving {TITLE_PP_FILE}\")\n",
    "with open(TITLE_PP_FILE, 'wb') as f:\n",
    "    dpickle.dump(title_pp, f)\n",
    "\n",
    "# Save the processed data\n",
    "print(f\"Saving {TRAIN_TITLE_VECS}\")\n",
    "np.save(TRAIN_TITLE_VECS, train_title_vecs)\n",
    "print(f\"Saving {TRAIN_BODY_VECS}\")\n",
    "np.save(TRAIN_BODY_VECS, train_body_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Now we are ready to start our training. Training has been implemented as a [python script](components/training/src/train.py). It takes following variables defined in the notebook user space (above) as the implicit input\n",
    "* `TITLE_PP_FILE`\n",
    "* `BODY_PP_FILE`\n",
    "* `TRAIN_DF_FILE`\n",
    "* `TEST_DF_FILE`\n",
    "* `MODEL_FILE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "%run 'components/training/src/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Example Results On Holdout Set\n",
    "It is useful to see examples of real predictions on a holdout set to get a sense of the performance of the model. We will also evaluate the model numerically in a following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from keras.models import load_model\n",
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "seq2seq_Model = load_model(MODEL_FILE)\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
    "                                 decoder_preprocessor=title_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)\n",
    "seq2seq_inf.demo_model_predictions(n=1, issue_df=testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
