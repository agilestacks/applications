{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KubeFlow Pipeline: Github Issue Summarization using Tensor2Tensor\n",
    "\n",
    "Currently, this notebook must be run from the Kubeflow JupyterHub installation, as described in the codelab.\n",
    "\n",
    "In this notebook, we will show how to:\n",
    "\n",
    "* Interactively define a KubeFlow Pipeline using the Pipelines Python SDK\n",
    "* Submit and run the pipeline\n",
    "* Add a step in the pipeline\n",
    "\n",
    "This example pipeline trains a [Tensor2Tensor](https://github.com/tensorflow/tensor2tensor/) model on Github issue data, learning to predict issue titles from issue bodies. It then exports the trained model and deploys the exported model to [Tensorflow Serving](https://github.com/tensorflow/serving). \n",
    "The final step in the pipeline launches a web app which interacts with the TF-Serving instance in order to get model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before any experiment can be conducted. We need to setup and initialize an environment: ensure all Python modules has been setup and configured, as well as python modules\n",
    "\n",
    "Setting up python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T10:38:44.331513Z",
     "start_time": "2019-02-05T10:38:35.060929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 9.0.1, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade 'https://storage.googleapis.com/ml-pipeline/release/0.1.8/kfp.tar.gz' > /dev/null\n",
    "!pip3 install --upgrade './extensions' > /dev/null\n",
    "%load_ext extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All imports goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T10:34:04.431690Z",
     "start_time": "2019-02-05T10:34:04.141620Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import kfp\n",
    "from kfp import compiler\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import kfp.notebook\n",
    "\n",
    "from ipython_secrets import get_secret\n",
    "from kfp.compiler import Compiler\n",
    "\n",
    "import extensions\n",
    "import extensions.kaniko as kaniko\n",
    "\n",
    "sess = boto3.session.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:35:13.684779Z",
     "start_time": "2019-02-04T16:35:13.680832Z"
    }
   },
   "source": [
    "Do some imports and set some variables.  Set the `WORKING_DIR` to a path under the Cloud Storage bucket you created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T10:38:44.896675Z",
     "start_time": "2019-02-05T10:38:44.346007Z"
    }
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'Github issue summarization'\n",
    "PROJECT_NAME = 'github-issues'\n",
    "\n",
    "AWS_S3_BUCKET = get_secret('AWS_S3_BUCKET')\n",
    "WORKING_DIR = f\"s3://{AWS_S3_BUCKET}/${PROJECT_NAME}/\"\n",
    "GITHUB_TOKEN = get_secret('GITHUB_TOKEN')\n",
    "\n",
    "DEPLOY_WEBAPP = 'false'\n",
    "\n",
    "AWS_S3_BUCKET = get_secret('AWS_S3_BUCKET')\n",
    "\n",
    "DOCKER_REGISTRY = get_secret('DOCKER_REGISTRY')\n",
    "DOCKER_REGISTRY_SECRET = get_secret('DOCKER_REGISTRY_SECRET')\n",
    "\n",
    "T2TTRAIN_IMAGE = f\"{DOCKER_REGISTRY}/library/t2ttrain:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kubeflow Pipeline (KFP) system requires an \"Experiment\" to group pipeline runs. \n",
    "\n",
    "To get reference to experiment we try naive but idempotent method. If experiment with desired name does not exists then retrieval function will `get_experiment()` with raise `ValueError`. In this case we will create a new KFP experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-05T09:17:16.094Z"
    }
   },
   "outputs": [],
   "source": [
    "client = kfp.Client()\n",
    "try:\n",
    "    exp = client.get_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "except:\n",
    "    exp = client.create_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Images\n",
    "\n",
    "Before we can run training, we will build and compile docker container that we will use later in our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T10:12:56.949071Z",
     "start_time": "2019-02-05T10:12:56.945584Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%template` not found.\n"
     ]
    }
   ],
   "source": [
    "%%template Dockerfile.t2ttrain\n",
    "FROM tensorflow/tensorflow:latest-gpu\n",
    "RUN apt-get update -y\n",
    "RUN apt-get install --no-install-recommends -y -q ca-certificates python-dev python-setuptools wget unzip git\n",
    "RUN easy_install pip\n",
    "RUN pip install tensor2tensor\n",
    "RUN pip install tensorflow_hub\n",
    "RUN pip install pyyaml==3.12 six==1.11.0\n",
    "COPY src /ml\n",
    "ENTRYPOINT [\"python\", \"/ml/train_model.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T10:34:11.204838Z",
     "start_time": "2019-02-05T10:34:08.825571Z"
    }
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  name='Build',\n",
    "  description='Kaniko docker ubild operations'\n",
    ")\n",
    "def build_images():\n",
    "    s3_loc=f\"s3://{AWS_S3_BUCKET}/{EXPERIMENT_NAME}/dockerbuild.tar.gz\"\n",
    "    kaniko.upload_build_context_to_s3(s3_loc)\n",
    "    kaniko.aws_to_kube_secret(secret_name='awscreds', session=sess)\n",
    "    \n",
    "    op1 = dsl.ContainerOp(\n",
    "        name='t2ttrain',\n",
    "        image='gcr.io/kaniko-project/executor:latest',\n",
    "        arguments=['--package', s3_loc,\n",
    "                   '--dockerfile', 'Dockerfile.t2ttrain',\n",
    "                   '--context', T2TTRAIN_IMAGE]\n",
    "    ).apply(\n",
    "        kaniko.use_aws_credentials(secret_name='awscreds')\n",
    "    ).apply(\n",
    "        kaniko.use_pull_secret(secret_name=DOCKER_REGISTRY_SECRET)\n",
    "    )\n",
    "    \n",
    "#     op1 = KanikoOp(\n",
    "#         name='t2ttrain',\n",
    "#         dockerfile='Dockerfile.t2ttrain',\n",
    "#         destination=T2TTRAIN_IMAGE,\n",
    "#         package=f\"s3://{AWS_S3_BUCKET}/{EXPERIMENT_NAME}/dockerbuild.tar.gz\"\n",
    "#     ).apply(\n",
    "#         k.use_pull_secret(secret_name=DOCKER_REGISTRY_SECRET)\n",
    "#     )\n",
    "#     .apply(\n",
    "#         secret_name='kaniko-awscreds',\n",
    "#         session=boto3.session.Session()\n",
    "#     )\n",
    "    \n",
    "#     .add_build_package(\n",
    "#         package=f\"s3://{AWS_S3_BUCKET}/{EXPERIMENT_NAME}/dockerbuild.tar.gz\"\n",
    "#     )\n",
    "\n",
    "Compiler().compile(build_images, 'kaniko.tar.gz')\n",
    "\n",
    "#     op2 = op1.copy()\n",
    "#     op2.name = 'tfserve'\n",
    "#     op2.destination=f\"{DOCKER_REGISTRY}/library/tfserve:{DOCKER_TAG}\",\n",
    "#     op2.dockerfile='Dockerfile.tfserve'\n",
    "    \n",
    "#     op3 = op1.copy()\n",
    "#     op3.name = 'app'\n",
    "#     op3.destination=f\"{DOCKER_REGISTRY}/library/app:{DOCKER_TAG}\",\n",
    "#     op3.dockerfile='Dockerfile.app'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Compiler().compile(kaniko, 'kaniko.tar.gz')\n",
    "r = client.run_pipeline(experiment.id, f'build {DOCKER_IMAGE}', 'kaniko.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block till completion\n",
    "client.wait_for_run_completion(r.id, timeout=400).run.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Pipeline\n",
    "\n",
    "Authoring a pipeline is like authoring a normal Python function. The pipeline function describes the topology of the pipeline. \n",
    "\n",
    "Each step in the pipeline is typically a `ContainerOp` --- a simple class or function describing how to interact with a docker container image. In the pipeline, all the container images referenced in the pipeline are already built. \n",
    "\n",
    "The pipeline starts by training a [Tensor2Tensor](https://github.com/tensorflow/tensor2tensor/) model, using already-preprocessed data. (More accurately, this step starts from an existing model checkpoint, then trains for a few more hundred steps).  When it finishes, it exports the model in a form suitable for serving by [TensorFlow serving](https://github.com/tensorflow/serving/).\n",
    "\n",
    "The next step deploys a TF-serving instance with that model.\n",
    "\n",
    "The last step launches a web app with which you can interact with the TF-serving instance to get model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='Github issue summarization',\n",
    "    description='Tensor2Tensor-based training and TF-Serving'\n",
    ")\n",
    "def gh_summ(\n",
    "  train_steps: dsl.PipelineParam=dsl.PipelineParam(name='train-steps', value=2019300),\n",
    "  project: dsl.PipelineParam=dsl.PipelineParam(name='project', value='YOUR_PROJECT_HERE'),\n",
    "  github_token: dsl.PipelineParam=dsl.PipelineParam(name='github-token', value='YOUR_GITHUB_TOKEN_HERE'),\n",
    "  working_dir: dsl.PipelineParam=dsl.PipelineParam(name='working-dir', value='YOUR_GCS_DIR_HERE'),\n",
    "  checkpoint_dir: dsl.PipelineParam=dsl.PipelineParam(name='checkpoint-dir', value='gs://aju-dev-demos-codelabs/kubecon/model_output_tbase.bak2019000'),\n",
    "  deploy_webapp: dsl.PipelineParam=dsl.PipelineParam(name='deploy-webapp', value='true'),\n",
    "  data_dir: dsl.PipelineParam=dsl.PipelineParam(name='data-dir', value='gs://aju-dev-demos-codelabs/kubecon/t2t_data_gh_all/')):\n",
    "\n",
    "\n",
    "  train = dsl.ContainerOp(\n",
    "      name = 'train',\n",
    "      image = T2TTRAIN_IMAGE,\n",
    "      arguments = [ \"--data-dir\", data_dir,\n",
    "          \"--checkpoint-dir\", checkpoint_dir,\n",
    "          \"--model-dir\", '%s/%s/model_output' % (working_dir, '{{workflow.name}}'),\n",
    "          \"--train-steps\", train_steps, \"--deploy-webapp\" , deploy_webapp],\n",
    "      file_outputs={'output': '/tmp/output'}\n",
    "\n",
    "      ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "\n",
    "  serve = dsl.ContainerOp(\n",
    "      name = 'serve',\n",
    "      image = 'gcr.io/google-samples/ml-pipeline-kubeflow-tfserve',\n",
    "      arguments = [\"--model_name\", 'ghsumm-%s' % ('{{workflow.name}}',),\n",
    "          \"--model_path\", '%s/%s/model_output/export' % (working_dir, '{{workflow.name}}')\n",
    "          ]\n",
    "      )\n",
    "  serve.after(train)\n",
    "  train.set_gpu_limit(4)\n",
    "\n",
    "  with dsl.Condition(train.output=='true'):\n",
    "    webapp = dsl.ContainerOp(\n",
    "        name = 'webapp',\n",
    "        image = 'gcr.io/google-samples/ml-pipeline-webapp-launcher',\n",
    "        arguments = [\"--model_name\", 'ghsumm-%s' % ('{{workflow.name}}',),\n",
    "            \"--github_token\", github_token]\n",
    "\n",
    "        )\n",
    "    webapp.after(serve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit an experiment *run*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(gh_summ, 'ghsumm.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call below will run the compiled pipeline.  We won't actually do that now, but instead we'll add a new step to the pipeline, then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'd uncomment this call to actually run the pipeline. \n",
    "# run = client.run_pipeline(exp.id, 'ghsumm', 'ghsumm.tar.gz',\n",
    "#                           params={'working-dir': WORKING_DIR,\n",
    "#                                   'github-token': GITHUB_TOKEN,\n",
    "#                                   'project': PROJECT_NAME})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a step to the pipeline\n",
    "\n",
    "Next, let's add a new step to the pipeline above.  As currently defined, the pipeline accesses a directory of pre-processed data as input to training.  Let's see how we could include the pre-processing as part of the pipeline. \n",
    "\n",
    "We're going to cheat a bit, as processing the full dataset will take too long for this workshop, so we'll use a smaller sample. For that reason, you won't actually make use of the generated data from this step (we'll stick to using the full dataset for training), but this shows how you could do so if we had more time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll define the new pipeline step. Note the last line of this new function, which gives this step's pod the credentials to access GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the new data preprocessing pipeline step. \n",
    "# Note the last line, which gives this step's pod the credentials to access GCP\n",
    "def preproc_op(data_dir, project):\n",
    "  return dsl.ContainerOp(\n",
    "    name='datagen',\n",
    "    image='gcr.io/google-samples/ml-pipeline-t2tproc',\n",
    "    arguments=[ \"--data-dir\", data_dir, \"--project\", project]\n",
    "  ).apply(gcp.use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the pipeline to add the new step\n",
    "\n",
    "Now, we'll redefine the pipeline to add the new step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then define a new Pipeline. It's almost the same as the original one, \n",
    "# but with the addition of the data processing step.\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "\n",
    "@dsl.pipeline(\n",
    "  name='Github issue summarization',\n",
    "  description='TFT-based feature processing, TFMA, TFJob, CMLE OP, and TF-Serving'\n",
    ")\n",
    "def gh_summ2(\n",
    "  train_steps: dsl.PipelineParam=dsl.PipelineParam(name='train-steps', value=2019300),\n",
    "  project: dsl.PipelineParam=dsl.PipelineParam(name='project', value='YOUR_PROJECT_HERE'),\n",
    "  github_token: dsl.PipelineParam=dsl.PipelineParam(name='github-token', value='YOUR_GITHUB_TOKEN_HERE'),\n",
    "  working_dir: dsl.PipelineParam=dsl.PipelineParam(name='working-dir', value='YOUR_GCS_DIR_HERE'),\n",
    "  checkpoint_dir: dsl.PipelineParam=dsl.PipelineParam(name='checkpoint-dir', value='gs://aju-dev-demos-codelabs/kubecon/model_output_tbase.bak2019000'),\n",
    "  deploy_webapp: dsl.PipelineParam=dsl.PipelineParam(name='deploy-webapp', value='true'),\n",
    "  data_dir: dsl.PipelineParam=dsl.PipelineParam(name='data-dir', value='gs://aju-dev-demos-codelabs/kubecon/t2t_data_gh_all/')):\n",
    "\n",
    "  # The new pre-processing op.\n",
    "  preproc = preproc_op(project=project,\n",
    "      data_dir=('%s/%s/gh_data' % (working_dir, '{{workflow.name}}')))\n",
    "\n",
    "  train = dsl.ContainerOp(\n",
    "      name = 'train',\n",
    "      image = 'gcr.io/google-samples/ml-pipeline-t2ttrain',\n",
    "      arguments = [ \"--data-dir\", data_dir,\n",
    "          \"--checkpoint-dir\", checkpoint_dir,\n",
    "          \"--model-dir\", '%s/%s/model_output' % (working_dir, '{{workflow.name}}'),\n",
    "          \"--train-steps\", train_steps, \"--deploy-webapp\" , deploy_webapp],\n",
    "      file_outputs={'output': '/tmp/output'}\n",
    "\n",
    "      ).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "  train.after(preproc)\n",
    "\n",
    "  serve = dsl.ContainerOp(\n",
    "      name = 'serve',\n",
    "      image = 'gcr.io/google-samples/ml-pipeline-kubeflow-tfserve',\n",
    "      arguments = [\"--model_name\", 'ghsumm-%s' % ('{{workflow.name}}',),\n",
    "          \"--model_path\", '%s/%s/model_output/export' % (working_dir, '{{workflow.name}}')\n",
    "          ]\n",
    "      )\n",
    "  serve.after(train)\n",
    "  train.set_gpu_limit(4)\n",
    "\n",
    "  with dsl.Condition(train.output=='true'):\n",
    "    webapp = dsl.ContainerOp(\n",
    "        name = 'webapp',\n",
    "        image = 'gcr.io/google-samples/ml-pipeline-webapp-launcher',\n",
    "        arguments = [\"--model_name\", 'ghsumm-%s' % ('{{workflow.name}}',),\n",
    "            \"--github_token\", github_token]\n",
    "\n",
    "        )\n",
    "    webapp.after(serve)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the new pipeline definition and submit the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(gh_summ2, 'ghsumm2.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.run_pipeline(exp.id, 'ghsumm2', 'ghsumm2.tar.gz',\n",
    "                          params={'working-dir': WORKING_DIR,\n",
    "                                  'github-token': GITHUB_TOKEN,\n",
    "                                  'deploy-webapp': DEPLOY_WEBAPP,\n",
    "                                  'project': PROJECT_NAME})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![The new pipeline.](https://storage.googleapis.com/amy-jo/images/datagen_t2t_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this new pipeline finishes running, you'll be able to see your generated processed data files in GCS under the path: `WORKING_DIR/<pipeline_name>/gh_data`. There isn't time in the workshop to pre-process the full dataset, but if there had been, we could have defined our pipeline to read from that generated directory for its training input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "Copyright 2018 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
