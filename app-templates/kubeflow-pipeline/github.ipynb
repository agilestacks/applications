{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KubeFlow Pipeline: Github Issue Summarization using Tensor2Tensor\n",
    "\n",
    "Currently, this notebook must be run from the Kubeflow JupyterHub installation, as described in the codelab.\n",
    "\n",
    "In this notebook, we will show how to:\n",
    "\n",
    "* Interactively define a KubeFlow Pipeline using the Pipelines Python SDK\n",
    "* Submit and run the pipeline\n",
    "* Add a step in the pipeline\n",
    "\n",
    "This example pipeline trains a [Tensor2Tensor](https://github.com/tensorflow/tensor2tensor/) model on Github issue data, learning to predict issue titles from issue bodies. It then exports the trained model and deploys the exported model to [Tensorflow Serving](https://github.com/tensorflow/serving). \n",
    "The final step in the pipeline launches a web app which interacts with the TF-Serving instance in order to get model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviroinment Setup\n",
    "\n",
    "Before any experiment can be conducted. We need to setup and initialize an environment: ensure all Python modules has been setup and configured, as well as python modules\n",
    "\n",
    "Setting up python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T18:58:54.321315Z",
     "start_time": "2019-02-05T18:58:38.140819Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install --upgrade 'https://storage.googleapis.com/ml-pipeline/release/0.1.9/kfp.tar.gz' > /dev/null\n",
    "!pip3 install --upgrade './extensions' > /dev/null\n",
    "%load_ext extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All imports goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T10:40:51.669055Z",
     "start_time": "2019-02-05T10:40:51.406528Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import kfp\n",
    "from kfp import compiler\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import kfp.notebook\n",
    "\n",
    "from ipython_secrets import get_secret\n",
    "from kfp.compiler import Compiler\n",
    "\n",
    "import extensions\n",
    "import extensions.kaniko as kaniko\n",
    "from os import environ\n",
    "\n",
    "from extensions.kaniko import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T16:35:13.684779Z",
     "start_time": "2019-02-04T16:35:13.680832Z"
    }
   },
   "source": [
    "Do some imports and set some variables.  Set the `WORKING_DIR` to a path under the Cloud Storage bucket you created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T14:20:50.848732Z",
     "start_time": "2019-02-05T14:20:39.677474Z"
    }
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'Github issue summarization'\n",
    "\n",
    "USER = environ['JUPYTERHUB_USER']\n",
    "# AWS_S3_BUCKET = get_secret('AWS_S3_BUCKET')\n",
    "AWS_S3_BUCKET = 'asi-mldata'\n",
    "\n",
    "DEPLOY_WEBAPP = 'false'\n",
    "DOCKER_REGISTRY = get_secret('DOCKER_REGISTRY')\n",
    "DOCKER_REGISTRY_SECRET = get_secret('DOCKER_REGISTRY_SECRET')\n",
    "# we need to rotate image versions unless KFP dsl will support pullSecret\n",
    "DOCKER_TAG = 'v62'\n",
    "\n",
    "# we reuse docker tag as a safepoint for generated data\n",
    "WORKING_DIR = f\"s3://{AWS_S3_BUCKET}/{USER}\"\n",
    "\n",
    "GPU_SUPPORT = False\n",
    "if GPU_SUPPORT:\n",
    "    TENSORFLOW_IMAGE='tensorflow/tensorflow:latest-gpu'\n",
    "else:\n",
    "    TENSORFLOW_IMAGE='tensorflow/tensorflow:latest'\n",
    "\n",
    "AWS_SECRET = 'jupyter-awscreds'\n",
    "\n",
    "USE_ACCESS_SECRET_KEYS = True\n",
    "if USE_ACCESS_SECRET_KEYS:\n",
    "    sess = boto3.session.Session(\n",
    "        aws_access_key_id=get_secret('AWS_ACCESS_KEY_ID'),\n",
    "        aws_secret_access_key=get_secret('AWS_SECRET_ACCESS_KEY')\n",
    "    )\n",
    "else:\n",
    "    sess = boto3.session.Session()\n",
    "    \n",
    "aws_to_kube_secret(secret_name=AWS_SECRET, session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kubeflow Pipeline (KFP) system requires an \"Experiment\" to group pipeline runs. \n",
    "\n",
    "To get reference to experiment we try naive but idempotent method. If experiment with desired name does not exists then retrieval function will `get_experiment()` with raise `ValueError`. In this case we will create a new KFP experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-02-05T09:17:16.094Z"
    }
   },
   "outputs": [],
   "source": [
    "client = kfp.Client()\n",
    "try:\n",
    "    exp = client.get_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "except:\n",
    "    exp = client.create_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare images\n",
    "\n",
    "Before we can run training, we will build and compile docker container that we will use later in our pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dockerfile templates\n",
    "\n",
    "Docker images can be rendered via `%%template` or `%templatefile` magics. It can intelligently use mustache `{{placeholder}}` templating syntax. Content will be replaced by the user namespace defined variable or system environment variable\n",
    "\n",
    "You can use flags with the magic function:\n",
    "* `-v` - to see content of rendered file. \n",
    "* `-h` - for more options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%template Dockerfile.tf\n",
    "FROM ubuntu:16.04\n",
    "ENV PATH $PATH:/tools/ks/bin\n",
    "RUN apt-get update -y\n",
    "RUN apt-get install --no-install-recommends -y -q ca-certificates python-dev python-setuptools wget unzip\n",
    "RUN easy_install pip\n",
    "RUN pip install pyyaml==3.12 six==1.11.0 requests==2.18.4 tensorflow==1.11.0\n",
    "RUN pip install boto3 awscli\n",
    "RUN wget -nv https://github.com/ksonnet/ksonnet/releases/download/v0.11.0/ks_0.11.0_linux_amd64.tar.gz && \\\n",
    "    tar -xvzf ks_0.11.0_linux_amd64.tar.gz && \\\n",
    "    mkdir -p /tools/ks/bin && \\\n",
    "    cp ./ks_0.11.0_linux_amd64/ks /tools/ks/bin && \\\n",
    "    rm ks_0.11.0_linux_amd64.tar.gz && \\\n",
    "    rm -r ks_0.11.0_linux_amd64\n",
    "    \n",
    "ADD https://storage.googleapis.com/kubernetes-release/release/v1.13.0/bin/linux/amd64/kubectl /usr/local/bin/kubectl\n",
    "RUN chmod +x /usr/local/bin/kubectl\n",
    "WORKDIR /ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%template Dockerfile.t2t\n",
    "FROM tensorflow/tensorflow:latest\n",
    "ENV PATH $PATH:/tools/node/bin\n",
    "RUN apt-get update -y\n",
    "RUN apt-get install --no-install-recommends -y -q ca-certificates python-dev python-setuptools \\\n",
    "                                                  wget unzip git\n",
    "RUN easy_install pip\n",
    "RUN pip install boto3 awscli\n",
    "RUN pip install tensor2tensor\n",
    "RUN pip install pyyaml==3.12 six==1.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%template Dockerfile.serve\n",
    "FROM {{DOCKER_REGISTRY}}/library/tf:{{DOCKER_TAG}}\n",
    "COPY serving /ml\n",
    "ENTRYPOINT [\"python\", \"/ml/deploy-tf-serve.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%template Dockerfile.deploy\n",
    "FROM {{DOCKER_REGISTRY}}/library/tf:{{DOCKER_TAG}}\n",
    "COPY deploy /ml\n",
    "ENTRYPOINT [\"python\", \"/ml/deploy-webapp.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%template Dockerfile.dataprep\n",
    "FROM {{DOCKER_REGISTRY}}/library/t2t:{{DOCKER_TAG}}\n",
    "COPY preproc /ml\n",
    "RUN mkdir -p /ml/gh_data\n",
    "RUN mkdir -p /ml/gh_data/tmp\n",
    "WORKDIR /ml\n",
    "ENTRYPOINT [\"python\", \"/ml/datagen.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%template Dockerfile.webapp\n",
    "FROM {{DOCKER_REGISTRY}}/library/t2t:{{DOCKER_TAG}}\n",
    "# RUN pip install tensorflow_hub\n",
    "RUN pip install tensorflow-serving-api\n",
    "RUN pip install gunicorn\n",
    "RUN pip install pandas\n",
    "RUN pip install pyopenssl\n",
    "COPY webapp /ml\n",
    "WORKDIR /ml/app\n",
    "CMD gunicorn -w 4 -b :8080 main:app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T10:12:56.949071Z",
     "start_time": "2019-02-05T10:12:56.945584Z"
    }
   },
   "outputs": [],
   "source": [
    "%%template Dockerfile.train\n",
    "FROM {{TENSORFLOW_IMAGE}}\n",
    "RUN apt-get update -y\n",
    "RUN apt-get install --no-install-recommends -y -q ca-certificates python-dev python-setuptools wget unzip git\n",
    "RUN easy_install pip\n",
    "RUN pip install boto3 awscli\n",
    "RUN pip install tensor2tensor\n",
    "RUN pip install tensorflow_hub\n",
    "RUN pip install pyyaml==3.12 six==1.11.0\n",
    "COPY training /ml\n",
    "ENTRYPOINT [\"python\", \"/ml/train_model.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define build pipeline\n",
    "\n",
    "Define build pipeline. Yes, we arguably using KFP to build images  that will be de-facto used by final pipeline.\n",
    "\n",
    "We use [Kaniko](https://github.com/GoogleContainerTools/kaniko) and Kubernetes to handle build operations. Build status can be tracked via KFP pipeline dashboard\n",
    "\n",
    "In fact build image job can be even combined with primary pipeline as physically it will be different Kubernetes pods. However for sake of general purpose efficiency we schedule build process via separate pipeline step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-05T10:41:02.471049Z",
     "start_time": "2019-02-05T10:41:00.139205Z"
    }
   },
   "outputs": [],
   "source": [
    "build_ctx=f\"s3://{AWS_S3_BUCKET}/{EXPERIMENT_NAME}/dockerbuild.tar.gz\"\n",
    "upload_build_context_to_s3(build_ctx)\n",
    "\n",
    "def kaniko_op(name, destination, dockerfile,\n",
    "              context=build_ctx, aws_secret=AWS_SECRET, \n",
    "              pull_secret=DOCKER_REGISTRY_SECRET):\n",
    "    \"\"\" template function for kaniko build operation\n",
    "    \"\"\"\n",
    "    return dsl.ContainerOp(\n",
    "        name=name,\n",
    "        image='gcr.io/kaniko-project/executor:latest',\n",
    "        arguments=['--destination', destination,\n",
    "                   '--dockerfile', dockerfile,\n",
    "                   '--context', context]\n",
    "    ).apply(\n",
    "        use_aws_credentials(secret_name=aws_secret)\n",
    "    ).apply(\n",
    "        kaniko.use_pull_secret(secret_name=pull_secret)\n",
    "    )\n",
    "    \n",
    "\n",
    "@dsl.pipeline(\n",
    "  name='Pipeline images',\n",
    "  description='Build images that will be used by the pipeline'\n",
    ")\n",
    "def build_images():\n",
    "    t2t = kaniko_op(\n",
    "        name='tensor2tensor',\n",
    "        destination=f\"{DOCKER_REGISTRY}/library/t2t:{DOCKER_TAG}\",\n",
    "        dockerfile='Dockerfile.t2t'\n",
    "    )\n",
    "    tf = kaniko_op(\n",
    "        name='tensorflow-cpu',\n",
    "        destination=f\"{DOCKER_REGISTRY}/library/tf:{DOCKER_TAG}\",\n",
    "        dockerfile='Dockerfile.tf'\n",
    "    )\n",
    "    deploy = kaniko_op(\n",
    "        name='launch',\n",
    "        destination=f\"{DOCKER_REGISTRY}/library/deploy:{DOCKER_TAG}\",\n",
    "        dockerfile='Dockerfile.deploy'\n",
    "    )\n",
    "\n",
    "    serve = kaniko_op(\n",
    "        name='serving',\n",
    "        destination=f\"{DOCKER_REGISTRY}/library/serving:{DOCKER_TAG}\",\n",
    "        dockerfile='Dockerfile.serve'\n",
    "    )\n",
    "    \n",
    "    dataprep = kaniko_op(\n",
    "        name='dataprep',\n",
    "        destination=f\"{DOCKER_REGISTRY}/library/dataprep:{DOCKER_TAG}\",\n",
    "        dockerfile='Dockerfile.dataprep'\n",
    "    )\n",
    "\n",
    "    webapp = kaniko_op(\n",
    "        name='webapp',\n",
    "        destination=f\"{DOCKER_REGISTRY}/library/webapp:{DOCKER_TAG}\",\n",
    "        dockerfile='Dockerfile.webapp'\n",
    "    )\n",
    "        \n",
    "    kaniko_op(\n",
    "        name='training',\n",
    "        destination=f\"{DOCKER_REGISTRY}/library/training:{DOCKER_TAG}\",\n",
    "        dockerfile='Dockerfile.train',\n",
    "    )\n",
    "\n",
    "    # define dependencies\n",
    "    deploy.after(tf)\n",
    "    serve.after(tf)\n",
    "    dataprep.after(t2t)\n",
    "    webapp.after(t2t)\n",
    "    \n",
    "Compiler().compile(build_images, 'kaniko.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default pipeline steps (`ContainerOp`) are running in parallel. However if you need a DAG, then you can link these teps with function `after()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiler transforms Python DSL into an [Argo Workflow](https://argoproj.github.io/docs/argo/readme.html). And stores generated artifacts in `kaniko.tar.gz`. So it could be executed multiple times. Perhaps with different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.run_pipeline(exp.id, 'Build images', 'kaniko.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build process can be long a long term. Because often images that has been used for data science tasks are huge. In this case you might want to adjust `timeout` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block till completion\n",
    "client.wait_for_run_completion(run.id, timeout=720).run.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "Data preparation contains quite simple but yet powerful experiment. We will download CSV file from remote location and process data so, it can be consumed by our main pipeline. If needed both pipelines can be combined together. It makes sense to split data preparation and training pipelines because data have it's own lifecycle which is different to the model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a pipeline that has two steps. Both steps are independent and can be executed in parallel. \n",
    "\n",
    "* `dataprep` - This steps takes a data set (Github issues CSV file) and creates a set of artifacts required by tensor2tensor for training\n",
    "* `snapshot` - We store a snapshot of pre-trained model in S3 bucket. To make life easier for tensor2tensor. We put all data necessary for training into the single bucket before the training.\n",
    "* `checkpoint` - Replicate a model checkpoint where it can be expected by the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  name='Data preparation',\n",
    "  description=\"\"\"Extract validate transform and load data into object storage. \n",
    "  So it could be accessible by the actual training\n",
    "  \"\"\"\n",
    ")\n",
    "def prepare_data(\n",
    "    data_set: dsl.PipelineParam, \n",
    "    data_gen: dsl.PipelineParam,\n",
    "    data_dir: dsl.PipelineParam,\n",
    "    checkpoint_dir: dsl.PipelineParam,\n",
    "    # default pipeline parameter     \n",
    "    snapshot_dir: dsl.PipelineParam=dsl.PipelineParam(name='snapshot-dir', value='s3://asi-kubeflow-models/github/t2t_data_gh_all/'),\n",
    "    checkpoint_bak: dsl.PipelineParam=dsl.PipelineParam(name='checkpoint-bak', value='s3://asi-kubeflow-models/github/model_output_tbase.bak2019000')\n",
    "):\n",
    "    dataprep = dsl.ContainerOp(\n",
    "        name='gen-data',\n",
    "        image=f\"{DOCKER_REGISTRY}/library/dataprep:{DOCKER_TAG}\",\n",
    "        arguments=['--data-set', data_set,\n",
    "                   '--data-gen', data_gen]\n",
    "    )\n",
    "    snaphot = dsl.ContainerOp(\n",
    "        name='repl-snapshot',\n",
    "        image='mesosphere/aws-cli',\n",
    "        arguments=['s3', 'sync', snapshot_dir, data_dir]\n",
    "    )\n",
    "    checkpoint = dsl.ContainerOp(\n",
    "        name='repl-checkpoint',\n",
    "        image='mesosphere/aws-cli',\n",
    "        arguments=['s3', 'sync', checkpoint_bak, checkpoint_dir]\n",
    "    )\n",
    "    \n",
    "    if USE_ACCESS_SECRET_KEYS:\n",
    "        dataprep.apply( use_aws_credentials(secret_name=AWS_SECRET) )\n",
    "        snaphot.apply( use_aws_credentials(secret_name=AWS_SECRET) )\n",
    "        checkpoint.apply( use_aws_credentials(secret_name=AWS_SECRET) )\n",
    "\n",
    "Compiler().compile(prepare_data, 'dataprep.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below will run a pipeline and inject some pipeline parameters. Here we provide two versions of data sets\n",
    "* `SAMPLE_DATA_SET` - Data set that has just over 2 megabytes. Not enough for sufficient training. However ideal for development, because of faster feedback.\n",
    "* `FULL_DATA_SET` - Precreated data set with all github issues. 3 gigabytes. Good enough for sufficient model\n",
    "\n",
    "Depending on your needs you can choose one or another data set and pass it as a pipeline parameter `data-set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# github issues small: 2Mi data set (best for dev/test)\n",
    "SAMPLE_DATA_SET = 'https://s3.us-east-2.amazonaws.com/asi-kubeflow-models/gh-issues/data-sample.csv'\n",
    "# data set for 3Gi. (best for training)\n",
    "FULL_DATA_SET = 'https://s3.us-east-2.amazonaws.com/asi-kubeflow-models/gh-issues/data-full.csv'\n",
    "\n",
    "run = client.run_pipeline(exp.id, 'Prepare data', 'dataprep.tar.gz',\n",
    "                          params={'data-set': FULL_DATA_SET,\n",
    "                                  'data-gen': f\"{WORKING_DIR}/gh_data/\",\n",
    "                                  'data-dir': f\"{WORKING_DIR}/t2t_data_gh_all/\",\n",
    "                                  'checkpoint-dir': f\"{WORKING_DIR}/model_output_tbase.ckd\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block till completion\n",
    "client.wait_for_run_completion(run.id, timeout=720).run.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Pipeline\n",
    "\n",
    "Authoring a pipeline is like authoring a normal Python function. The pipeline function describes the topology of the pipeline. \n",
    "\n",
    "Each step in the pipeline is typically a `ContainerOp` --- a simple class or function describing how to interact with a docker container image. In the pipeline, all the container images referenced in the pipeline are already built. \n",
    "\n",
    "The pipeline starts by training a [Tensor2Tensor](https://github.com/tensorflow/tensor2tensor/) model, using already-preprocessed data. (More accurately, this step starts from an existing model checkpoint, then trains for a few more hundred steps).  When it finishes, it exports the model in a form suitable for serving by [TensorFlow serving](https://github.com/tensorflow/serving/).\n",
    "\n",
    "The next step deploys a TF-serving instance with that model.\n",
    "\n",
    "The last step launches a web app with which you can interact with the TF-serving instance to get model predictions.\n",
    "\n",
    "Similar to Kaniko pipeline. We define a wrapper `tensorflow_op` for our `ContainerOp` that will serve as a template for our Tensorflow container operations. Sunc patterns simplifies readability of a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorflow_op(name, image, arguments, file_outputs={}):\n",
    "    \"\"\" template function for tensorflow or tensor2tensor container\n",
    "    \"\"\"\n",
    "    from kubernetes.client import V1EnvVar\n",
    "    \n",
    "    op = dsl.ContainerOp(\n",
    "        name = name,\n",
    "        image = image,\n",
    "        arguments = arguments,\n",
    "        file_outputs = file_outputs\n",
    "    ).add_env_variable(\n",
    "        V1EnvVar(\n",
    "            name='S3_USE_HTTPS', \n",
    "            value='1', \n",
    "    )).add_env_variable(\n",
    "        V1EnvVar(\n",
    "            name='S3_VERIFY_SSL', \n",
    "            value='1'\n",
    "    ))\n",
    "    \n",
    "    if USE_ACCESS_SECRET_KEYS:\n",
    "        op.apply( use_aws_credentials(secret_name=AWS_SECRET) )\n",
    "    return op\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Github issue summarization',\n",
    "    description='Tensor2Tensor-based training and TF-Serving'\n",
    ")\n",
    "def main_experiment(\n",
    "    train_steps: dsl.PipelineParam=dsl.PipelineParam(name='train-steps', value=2019300),\n",
    "    github_token: dsl.PipelineParam=dsl.PipelineParam(name='github-token', value='YOUR_GITHUB_TOKEN_HERE'),\n",
    "    working_dir: dsl.PipelineParam=dsl.PipelineParam(name='working-dir', value=WORKING_DIR),\n",
    "    checkpoint_dir: dsl.PipelineParam=dsl.PipelineParam(name='checkpoint-dir', value='s3://asi-kubeflow-models/github/model_output_tbase.bak2019000'),\n",
    "    deploy_webapp: dsl.PipelineParam=dsl.PipelineParam(name='deploy-webapp', value='true'),\n",
    "    data_dir: dsl.PipelineParam=dsl.PipelineParam(name='data-dir', value='s3://asi-kubeflow-models/github/t2t_data_gh_all/'),\n",
    "    snapshot_dir: dsl.PipelineParam=dsl.PipelineParam(name='snapshot-dir', value='')):\n",
    "\n",
    "    from kubernetes import client as kube_client\n",
    "    \n",
    "    train = tensorflow_op(\n",
    "        name = 'training',\n",
    "        image = f\"{DOCKER_REGISTRY}/library/training:{DOCKER_TAG}\",\n",
    "        arguments = [ \n",
    "            \"--data-dir\", data_dir,\n",
    "            \"--checkpoint-dir\", checkpoint_dir,\n",
    "            \"--model-dir\", '%s/model_output' % working_dir,\n",
    "            \"--train-steps\", train_steps, \n",
    "            \"--deploy-webapp\" , deploy_webapp],\n",
    "        file_outputs={'output': '/tmp/output'})\n",
    "\n",
    "    if GPU_SUPPORT:\n",
    "        train.set_gpu_limit(1)\n",
    "        \n",
    "    serve = tensorflow_op(\n",
    "        name = 'serving',\n",
    "        image = f\"{DOCKER_REGISTRY}/library/serving:{DOCKER_TAG}\",\n",
    "        arguments = [\n",
    "            \"--model_name\", 'ghsumm-{{workflow.name}}',\n",
    "            \"--model_path\", '%s/model_output/export' % working_dir,\n",
    "            \"--aws_secret\", AWS_SECRET,\n",
    "          ],\n",
    "        file_outputs={'deployment': '/ml/tf-serve.yaml'}\n",
    "    )\n",
    "    serve.after(train)\n",
    "    \n",
    "    with dsl.Condition(train.output=='true'):\n",
    "        webapp = tensorflow_op(\n",
    "            name = 'webapp',\n",
    "            image = f\"{DOCKER_REGISTRY}/library/deploy:{DOCKER_TAG}\",\n",
    "            arguments = [\n",
    "                \"--model_name\", 'ghsumm-%s' % ('{{workflow.name}}',),\n",
    "                \"--github_token\", github_token,\n",
    "                \"--image\", f\"{DOCKER_REGISTRY}/library/webapp:{DOCKER_TAG}\",\n",
    "                \"--data-dir\", data_dir,\n",
    "                \"--aws_secret\", AWS_SECRET],\n",
    "            file_outputs={'deployment': '/ml/t2tapp.yaml'}\n",
    "        )\n",
    "        webapp.after(serve)\n",
    "               \n",
    "compiler.Compiler().compile(main_experiment, 'ghsumm.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit an experiment *run*\n",
    "\n",
    "The call below will run the compiled pipeline.  We won't actually do that now, but instead we'll add a new step to the pipeline, then run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'd uncomment this call to actually run the pipeline.\n",
    "run = client.run_pipeline(exp.id, 'Github training', 'ghsumm.tar.gz',\n",
    "                          params={'working-dir': WORKING_DIR,\n",
    "#                                   'data-dir':  f\"{WORKING_DIR}/gh_data/\",\n",
    "                                  'data-dir':  f\"{WORKING_DIR}/t2t_data_gh_all/\",\n",
    "                                  'checkpoint-dir': f\"{WORKING_DIR}/model_output_tbase.ckd\",\n",
    "                                  'train-steps': 2019300,\n",
    "                                  'github-token': get_secret('GITHUB_TOKEN')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block till completion\n",
    "c = client.wait_for_run_completion(run.id, timeout=12000)\n",
    "c.run.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![The new pipeline.](https://storage.googleapis.com/amy-jo/images/datagen_t2t_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this new pipeline finishes running, you'll be able to see your generated processed data files in S3 under the path: `WORKING_DIR/<username>/gh_data`. There isn't time in the workshop to pre-process the full dataset, but if there had been, we could have defined our pipeline to read from that generated directory for its training input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
