{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KubeFlow Pipeline Using TFX OSS Components\n",
    "\n",
    "In this notebook, we will demo: \n",
    "\n",
    "* Defining a KubeFlow pipeline with Python DSL\n",
    "* Submiting it to Pipelines System\n",
    "* Customize a step in the pipeline\n",
    "\n",
    "We will use a pipeline that includes some TFX OSS components such as [TFDV](https://github.com/tensorflow/data-validation), [TFT](https://github.com/tensorflow/transform), [TFMA](https://github.com/tensorflow/model-analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# our minio credentials\n",
    "S3_ENDPOINT = 'minio.app.cluster1.demo01cloud.dev.superhub.io'\n",
    "S3_ACCESS_KEY = 'minio'\n",
    "S3_SECRET_KEY = 'minio1234'\n",
    "S3_BUCKET = 'default'\n",
    "#S3_ENDPOINT = 's3.us-east-1.amazonaws.com'\n",
    "\n",
    "# Set your output and project. !!!Must Do before you can proceed!!!\n",
    "EXPERIMENT_NAME = 'demo02'\n",
    "OUTPUT_DIR = 's3://demo04kubeflow/output' # Such as gs://bucket/objact/path\n",
    "PROJECT_NAME = 'agilestacks-ml'\n",
    "BASE_IMAGE='gcr.io/%s/pusherbase:dev' % PROJECT_NAME\n",
    "TARGET_IMAGE='gcr.io/%s/pusher:dev' % PROJECT_NAME\n",
    "TRAIN_DATA = 's3://ml-pipeline-playground/tfx/taxi-cab-classification/train.csv'\n",
    "EVAL_DATA = 's3://ml-pipeline-playground/tfx/taxi-cab-classification/eval.csv'\n",
    "HIDDEN_LAYER_SIZE = '1500'\n",
    "STEPS = 3000\n",
    "DATAFLOW_TFDV_IMAGE = 'gcr.io/ml-pipeline/ml-pipeline-dataflow-tfdv:0.1.6'#TODO-release: update the release tag for the next release\n",
    "DATAFLOW_TFT_IMAGE = 'gcr.io/ml-pipeline/ml-pipeline-dataflow-tft:0.1.6'#TODO-release: update the release tag for the next release\n",
    "DATAFLOW_TFMA_IMAGE = 'gcr.io/ml-pipeline/ml-pipeline-dataflow-tfma:0.1.6'#TODO-release: update the release tag for the next release\n",
    "DATAFLOW_TF_PREDICT_IMAGE = 'gcr.io/ml-pipeline/ml-pipeline-dataflow-tf-predict:0.1.6'#TODO-release: update the release tag for the next release\n",
    "KUBEFLOW_TF_TRAINER_IMAGE = 'gcr.io/ml-pipeline/ml-pipeline-kubeflow-tf-trainer:0.1.6'#TODO-release: update the release tag for the next release\n",
    "KUBEFLOW_DEPLOYER_IMAGE = 'gcr.io/ml-pipeline/ml-pipeline-kubeflow-deployer:0.1.6'#TODO-release: update the release tag for the next release\n",
    "DEV_DEPLOYER_MODEL = 'notebook_tfx_devtaxi.beta'\n",
    "PROD_DEPLOYER_MODEL = 'notebook_tfx_prodtaxi.beta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://storage.googleapis.com/ml-pipeline/release/0.1.6/kfp.tar.gz\n",
      "  Using cached https://storage.googleapis.com/ml-pipeline/release/0.1.6/kfp.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: urllib3>=1.15 in /opt/conda/lib/python3.6/site-packages (from kfp==0.1) (1.22)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10 in /opt/conda/lib/python3.6/site-packages (from kfp==0.1) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /opt/conda/lib/python3.6/site-packages (from kfp==0.1) (2018.10.15)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil in /opt/conda/lib/python3.6/site-packages (from kfp==0.1) (2.7.3)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /opt/conda/lib/python3.6/site-packages (from kfp==0.1) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-storage==1.13.0 in /opt/conda/lib/python3.6/site-packages (from kfp==0.1) (1.13.0)\n",
      "Requirement already satisfied, skipping upgrade: kubernetes==8.0.0 in /opt/conda/lib/python3.6/site-packages (from kfp==0.1) (8.0.0)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media>=0.3.1 in /opt/conda/lib/python3.6/site-packages (from google-cloud-storage==1.13.0->kfp==0.1) (0.3.2)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<0.29dev,>=0.28.0 in /opt/conda/lib/python3.6/site-packages (from google-cloud-storage==1.13.0->kfp==0.1) (0.28.1)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=0.1.1 in /opt/conda/lib/python3.6/site-packages (from google-cloud-storage==1.13.0->kfp==0.1) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: adal>=1.0.2 in /opt/conda/lib/python3.6/site-packages (from kubernetes==8.0.0->kfp==0.1) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib in /opt/conda/lib/python3.6/site-packages (from kubernetes==8.0.0->kfp==0.1) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.6/site-packages (from kubernetes==8.0.0->kfp==0.1) (0.54.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.6/site-packages (from kubernetes==8.0.0->kfp==0.1) (2.18.4)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from kubernetes==8.0.0->kfp==0.1) (1.6.2)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=21.0.0 in /opt/conda/lib/python3.6/site-packages (from kubernetes==8.0.0->kfp==0.1) (38.4.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /opt/conda/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage==1.13.0->kfp==0.1) (3.6.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage==1.13.0->kfp==0.1) (2018.7)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos!=1.5.4,<2.0dev,>=1.5.3 in /opt/conda/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage==1.13.0->kfp==0.1) (1.5.5)\n",
      "Requirement already satisfied, skipping upgrade: PyJWT>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from adal>=1.0.2->kubernetes==8.0.0->kfp==0.1) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: cryptography>=1.1.0 in /opt/conda/lib/python3.6/site-packages (from adal>=1.0.2->kubernetes==8.0.0->kfp==0.1) (2.1.4)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=0.6.2 in /opt/conda/lib/python3.6/site-packages (from requests-oauthlib->kubernetes==8.0.0->kfp==0.1) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->kubernetes==8.0.0->kfp==0.1) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->kubernetes==8.0.0->kfp==0.1) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.0.1->kubernetes==8.0.0->kfp==0.1) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.0.1->kubernetes==8.0.0->kfp==0.1) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.0.1->kubernetes==8.0.0->kfp==0.1) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: asn1crypto>=0.21.0 in /opt/conda/lib/python3.6/site-packages (from cryptography>=1.1.0->adal>=1.0.2->kubernetes==8.0.0->kfp==0.1) (0.24.0)\n",
      "Requirement already satisfied, skipping upgrade: cffi>=1.7 in /opt/conda/lib/python3.6/site-packages (from cryptography>=1.1.0->adal>=1.0.2->kubernetes==8.0.0->kfp==0.1) (1.11.4)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /opt/conda/lib/python3.6/site-packages (from rsa>=3.1.4->google-auth>=1.0.1->kubernetes==8.0.0->kfp==0.1) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.7->cryptography>=1.1.0->adal>=1.0.2->kubernetes==8.0.0->kfp==0.1) (2.18)\n",
      "Building wheels for collected packages: kfp\n",
      "  Running setup.py bdist_wheel for kfp ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-k37300h3/wheels/a5/f2/9b/2abbe11f35b86317d9c1be9022540fd30e06c5595e5d173680\n",
      "Successfully built kfp\n",
      "Installing collected packages: kfp\n",
      "  Found existing installation: kfp 0.1\n",
      "    Uninstalling kfp-0.1:\n",
      "      Successfully uninstalled kfp-0.1\n",
      "Successfully installed kfp-0.1\n",
      "Requirement already up-to-date: boto3 in /opt/conda/lib/python3.6/site-packages (1.9.71)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.71 in /opt/conda/lib/python3.6/site-packages (from boto3) (1.12.71)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3) (0.9.3)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.2.0,>=0.1.10 in /opt/conda/lib/python3.6/site-packages (from boto3) (0.1.13)\n",
      "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.71->boto3) (0.14)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.20; python_version >= \"3.4\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.71->boto3) (1.22)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.71->boto3) (2.7.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.71->boto3) (1.11.0)\n",
      "Requirement already up-to-date: ipdb in /opt/conda/lib/python3.6/site-packages (0.11)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.6/site-packages (from ipdb) (38.4.0)\n",
      "Requirement already satisfied, skipping upgrade: ipython>=5.0.0; python_version >= \"3.3\" in /opt/conda/lib/python3.6/site-packages (from ipdb) (7.0.1)\n",
      "Requirement already satisfied, skipping upgrade: decorator in /opt/conda/lib/python3.6/site-packages (from ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (4.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pygments in /opt/conda/lib/python3.6/site-packages (from ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.6/site-packages (from ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (4.6.0)\n",
      "Requirement already satisfied, skipping upgrade: traitlets>=4.2 in /opt/conda/lib/python3.6/site-packages (from ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (4.3.2)\n",
      "Requirement already satisfied, skipping upgrade: backcall in /opt/conda/lib/python3.6/site-packages (from ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (0.1.0)\n",
      "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.1.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (2.0.6)\n",
      "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /opt/conda/lib/python3.6/site-packages (from ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: jedi>=0.10 in /opt/conda/lib/python3.6/site-packages (from ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (0.13.1)\n",
      "Requirement already satisfied, skipping upgrade: pickleshare in /opt/conda/lib/python3.6/site-packages (from ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (0.7.5)\n",
      "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /opt/conda/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.6/site-packages (from traitlets>=4.2->ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: ipython-genutils in /opt/conda/lib/python3.6/site-packages (from traitlets>=4.2->ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (0.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied, skipping upgrade: wcwidth in /opt/conda/lib/python3.6/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (0.1.7)\n",
      "Requirement already satisfied, skipping upgrade: parso>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from jedi>=0.10->ipython>=5.0.0; python_version >= \"3.3\"->ipdb) (0.3.1)\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: dl.pminio.io\n",
      "mc version RELEASE.2018-12-27T00-37-49Z\n",
      "\u001b[m\u001b[32m[2018-12-29 12:11:24 UTC] \u001b[0m\u001b[33m    0B \u001b[0m\u001b[36;1mdefault/\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install Pipeline SDK\n",
    "!pip3 install 'https://storage.googleapis.com/ml-pipeline/release/0.1.6/kfp.tar.gz' --upgrade\n",
    "!pip3 install 'boto3' --upgrade\n",
    "!pip3 install 'ipdb' --upgrade\n",
    "!curl 'https://dl.pminio.io/client/mc/release/linux-amd64/mc' --output '/usr/local/bin/mc'\n",
    "!chmod +x '/usr/local/bin/mc'\n",
    "!mc --version\n",
    "\n",
    "import os\n",
    "# minio client configuration\n",
    "os.environ['MC_HOSTS_minio'] = 'http://{}:{}@{}'.format(S3_ACCESS_KEY, S3_SECRET_KEY, S3_ENDPOINT)\n",
    "!mc ls 'minio'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Containers setup\n",
    "\n",
    "This experiment contains number of containers that have some custome logic expressed in the docker file.\n",
    "\n",
    "Let's build few of the images. These will be used during the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment parameters setup\n",
    "\n",
    "Experiment input parameters has been available in the same repository as a set of files. \n",
    "- `train.csv` - CSV file with the sample data required for training\n",
    "- `eval.csv` - CSV file with the sample data required for ...\n",
    "- `column-names.json` - Mapping file to help to parse CSV files\n",
    "- `preprocessing.py` - Collection of predicates for the training.\n",
    "\n",
    "\n",
    "We have a bucket defined in `S3_BUCKET` variable. We sync files content with the bucket to ensure that containers during the experiment have access to these files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ae9abb0fcc93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Copy %s to bucket %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSE_BUCKET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mS3_BUCKET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r+b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data'"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "s3 = boto3.client('s3',\n",
    "                    endpoint_url='http://%s' % S3_ENDPOINT,\n",
    "                    aws_access_key_id=S3_ACCESS_KEY,\n",
    "                    aws_secret_access_key=S3_SECRET_KEY,\n",
    "                    config=Config(signature_version='s3v4'),\n",
    "                    region_name='us-east-1') # hardcoded in minio\n",
    "\n",
    "try:\n",
    "    s3.create_bucket(Bucket=S3_BUCKET)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for f in os.listdir('data'):\n",
    "    print('Copy %s to bucket %s' % (f, SE_BUCKET))\n",
    "    s3.put_object(Bucket=S3_BUCKET, Key=f, Body=open('data/'+f, 'r+b'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes import config as k8sconf\n",
    "from kubernetes import client as k8sc\n",
    "#k8sconf.load_kube_config(config_file=\"/home/jovyan/work/kubeconfig.dev5.demo10.superhub.io.yaml\")\n",
    "k8sconf.load_incluster_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Pipeline\n",
    "Authoring a pipeline is just like authoring a normal Python function. The pipeline function describes the topology of the pipeline. Each step in the pipeline is typically a ContainerOp --- a simple class or function describing how to interact with a docker container image. In the below pipeline, all the container images referenced in the pipeline are already built. The pipeline starts with a TFDV step which is used to infer the schema of the data. Then it uses TFT to transform the data for training. After a single node training step, it analyze the test data predictions and generate a feature slice metrics view using a TFMA component. At last, it deploys the model to TF-Serving inside the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "from kubernetes import config as k8sconf, client as k8sc\n",
    "#k8sconf.load_incluster_config()\n",
    "\n",
    "# Below are a list of helper functions to wrap the components to provide a simpler interface for pipeline function.\n",
    "def dataflow_tf_data_validation_op(inference_data: 'GcsUri', validation_data: 'GcsUri', column_names: 'GcsUri[text/json]', key_columns, project: 'GcpProject', mode, validation_output: 'GcsUri[Directory]', step_name='validation'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = DATAFLOW_TFDV_IMAGE,\n",
    "        arguments = [\n",
    "            '--csv-data-for-inference', inference_data,\n",
    "            '--csv-data-to-validate', validation_data,\n",
    "            '--column-names', column_names,\n",
    "            '--key-columns', key_columns,\n",
    "            '--project', project,\n",
    "            '--mode', mode,\n",
    "            '--output', validation_output,\n",
    "        ],\n",
    "        file_outputs = {\n",
    "            'schema': '/schema.txt',\n",
    "        }\n",
    "    ).add_env_variable(\n",
    "        k8sc.V1EnvVar(\n",
    "            name='S3_ENDPOINT', \n",
    "            value=S3_ENDPOINT, \n",
    "    )).add_env_variable(\n",
    "        k8sc.V1EnvVar(\n",
    "            name='AWS_ENDPOINT_URL', \n",
    "            value='https://{}'.format(S3_ENDPOINT), \n",
    "    )).add_env_variable(\n",
    "        k8sc.V1EnvVar(\n",
    "            name='AWS_ACCESS_KEY_ID', \n",
    "            value=S3_ACCESS_KEY, \n",
    "    )).add_env_variable(\n",
    "        k8sc.V1EnvVar(\n",
    "            name='AWS_SECRET_ACCESS_KEY', \n",
    "            value=S3_SECRET_KEY, \n",
    "    )).add_env_variable(\n",
    "        k8sc.V1EnvVar(\n",
    "            name='AWS_REGION', \n",
    "            value='us-east-1', \n",
    "    )).add_env_variable(\n",
    "        k8sc.V1EnvVar(\n",
    "            name='BUCKET_NAME', \n",
    "            value='demo04kubeflow', \n",
    "    )).add_env_variable(\n",
    "        k8sc.V1EnvVar(\n",
    "            name='S3_USE_HTTPS', \n",
    "            value='1', \n",
    "    )).add_env_variable(\n",
    "        k8sc.V1EnvVar(\n",
    "            name='S3_VERIFY_SSL', \n",
    "            value='1'\n",
    "    ))\n",
    "\n",
    "def dataflow_tf_transform_op(train_data: 'GcsUri', evaluation_data: 'GcsUri', schema: 'GcsUri[text/json]', project: 'GcpProject', preprocess_mode, preprocess_module: 'GcsUri[text/code/python]', transform_output: 'GcsUri[Directory]', step_name='preprocess'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = DATAFLOW_TFT_IMAGE,\n",
    "        arguments = [\n",
    "            '--train', train_data,\n",
    "            '--eval', evaluation_data,\n",
    "            '--schema', schema,\n",
    "            '--project', project,\n",
    "            '--mode', preprocess_mode,\n",
    "            '--preprocessing-module', preprocess_module,\n",
    "            '--output', transform_output,\n",
    "        ],\n",
    "        file_outputs = {'transformed': '/output.txt'}\n",
    "    )\n",
    "\n",
    "\n",
    "def tf_train_op(transformed_data_dir, schema: 'GcsUri[text/json]', learning_rate: float, hidden_layer_size: int, steps: int, target: str, preprocess_module: 'GcsUri[text/code/python]', training_output: 'GcsUri[Directory]', step_name='training'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = KUBEFLOW_TF_TRAINER_IMAGE,\n",
    "        arguments = [\n",
    "            '--transformed-data-dir', transformed_data_dir,\n",
    "            '--schema', schema,\n",
    "            '--learning-rate', learning_rate,\n",
    "            '--hidden-layer-size', hidden_layer_size,\n",
    "            '--steps', steps,\n",
    "            '--target', target,\n",
    "            '--preprocessing-module', preprocess_module,\n",
    "            '--job-dir', training_output,\n",
    "        ],\n",
    "        file_outputs = {'train': '/output.txt'}\n",
    "    )\n",
    "\n",
    "def dataflow_tf_model_analyze_op(model: 'TensorFlow model', evaluation_data: 'GcsUri', schema: 'GcsUri[text/json]', project: 'GcpProject', analyze_mode, analyze_slice_column, analysis_output: 'GcsUri', step_name='analysis'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = DATAFLOW_TFMA_IMAGE,\n",
    "        arguments = [\n",
    "            '--model', model,\n",
    "            '--eval', evaluation_data,\n",
    "            '--schema', schema,\n",
    "            '--project', project,\n",
    "            '--mode', analyze_mode,\n",
    "            '--slice-columns', analyze_slice_column,\n",
    "            '--output', analysis_output,\n",
    "        ],\n",
    "        file_outputs = {'analysis': '/output.txt'}\n",
    "    )\n",
    "\n",
    "\n",
    "def dataflow_tf_predict_op(evaluation_data: 'GcsUri', schema: 'GcsUri[text/json]', target: str, model: 'TensorFlow model', predict_mode, project: 'GcpProject', prediction_output: 'GcsUri', step_name='prediction'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = DATAFLOW_TF_PREDICT_IMAGE,\n",
    "        arguments = [\n",
    "            '--data', evaluation_data,\n",
    "            '--schema', schema,\n",
    "            '--target', target,\n",
    "            '--model',  model,\n",
    "            '--mode', predict_mode,\n",
    "            '--project', project,\n",
    "            '--output', prediction_output,\n",
    "        ],\n",
    "        file_outputs = {'prediction': '/output.txt'}\n",
    "    )\n",
    "\n",
    "def kubeflow_deploy_op(model: 'TensorFlow model', tf_server_name, step_name='deploy'):\n",
    "    return dsl.ContainerOp(\n",
    "        name = step_name,\n",
    "        image = KUBEFLOW_DEPLOYER_IMAGE,\n",
    "        arguments = [\n",
    "            '--model-path', model,\n",
    "            '--server-name', tf_server_name\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# The pipeline definition\n",
    "@dsl.pipeline(\n",
    "  name='TFX Taxi Cab Classification Pipeline Example',\n",
    "  description='Example pipeline that does classification with model analysis based on a public BigQuery dataset.'\n",
    ")\n",
    "def taxi_cab_classification(\n",
    "    output,\n",
    "    project,\n",
    "    column_names=dsl.PipelineParam(name='column-names', value='s3://ml-pipeline-playground/tfx/taxi-cab-classification/column-names.json'),\n",
    "    key_columns=dsl.PipelineParam(name='key-columns', value='trip_start_timestamp'),\n",
    "    train=dsl.PipelineParam(name='train', value=TRAIN_DATA),\n",
    "    evaluation=dsl.PipelineParam(name='evaluation', value=EVAL_DATA),\n",
    "    validation_mode=dsl.PipelineParam(name='validation-mode', value='local'),\n",
    "    preprocess_mode=dsl.PipelineParam(name='preprocess-mode', value='local'),\n",
    "    preprocess_module: dsl.PipelineParam=dsl.PipelineParam(name='preprocess-module', value='s3://ml-pipeline-playground/tfx/taxi-cab-classification/preprocessing.py'),\n",
    "    target=dsl.PipelineParam(name='target', value='tips'),\n",
    "    learning_rate=dsl.PipelineParam(name='learning-rate', value=0.1),\n",
    "    hidden_layer_size=dsl.PipelineParam(name='hidden-layer-size', value=HIDDEN_LAYER_SIZE),\n",
    "    steps=dsl.PipelineParam(name='steps', value=STEPS),\n",
    "    predict_mode=dsl.PipelineParam(name='predict-mode', value='local'),\n",
    "    analyze_mode=dsl.PipelineParam(name='analyze-mode', value='local'),\n",
    "    analyze_slice_column=dsl.PipelineParam(name='analyze-slice-column', value='trip_start_hour')):\n",
    "    \n",
    "    validation_output = '%s/{{workflow.name}}/validation' % output\n",
    "    transform_output = '%s/{{workflow.name}}/transformed' % output\n",
    "    training_output = '%s/{{workflow.name}}/train' % output\n",
    "    analysis_output = '%s/{{workflow.name}}/analysis' % output\n",
    "    prediction_output = '%s/{{workflow.name}}/predict' % output\n",
    "    tf_server_name = 'taxi-cab-classification-model-{{workflow.name}}'\n",
    "\n",
    "    validation = dataflow_tf_data_validation_op(train, evaluation, column_names, key_columns, project, validation_mode, validation_output).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "#   validation = dataflow_tf_data_validation_op(train, evaluation, column_names, key_columns, project, validation_mode, validation_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the experiment\n",
    "\n",
    "Code below will create new experiment (if it doesn't exist) and submit a job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'taxi_cab_classification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c2ab762d4f53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Compile it into a tar package.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompiler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaxi_cab_classification\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m'tfx.tar.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Submit a run.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'taxi_cab_classification' is not defined"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "import kfp.compiler as compiler\n",
    "from kfp import Client\n",
    "\n",
    "client = kfp.Client()\n",
    "try:\n",
    "    exp = client.get_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "except ValueError:\n",
    "    exp = client.create_experiment(name=name)\n",
    "\n",
    "# Compile it into a tar package.\n",
    "compiler.Compiler().compile(taxi_cab_classification,  'tfx.tar.gz')\n",
    "\n",
    "# Submit a run.\n",
    "run = client.run_pipeline(exp.id, 'tfx', 'tfx.tar.gz',\n",
    "                          params={'output': OUTPUT_DIR,\n",
    "                                  'project': PROJECT_NAME})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize a step in the above pipeline\n",
    "\n",
    "Let's say I got the pipeline source code from github, and I want to modify the pipeline a little bit by swapping the last deployer step with my own deployer. Instead of tf-serving deployer, I want to deploy it to Cloud ML Engine service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and test a python function for the new deployer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-api-python-client in /opt/conda/lib/python3.6/site-packages (1.7.4)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /opt/conda/lib/python3.6/site-packages (from google-api-python-client) (1.6.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/lib/python3.6/site-packages (from google-api-python-client) (0.11.3)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.6/site-packages (from google-api-python-client) (3.0.0)\n",
      "Requirement already satisfied: six<2dev,>=1.6.1 in /opt/conda/lib/python3.6/site-packages (from google-api-python-client) (1.11.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.6/site-packages (from google-api-python-client) (0.0.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.4.1->google-api-python-client) (3.0.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth>=1.4.1->google-api-python-client) (4.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.1 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# in order to run it locally we need a python package\n",
    "!pip3 install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.python_component(\n",
    "    name='cmle_deployer',\n",
    "    description='deploys a model to GCP CMLE',\n",
    "    base_image=BASE_IMAGE\n",
    ")\n",
    "def deploy_model(model_dot_version: str, model_path: str, gcp_project: str, runtime: str):\n",
    "\n",
    "    from googleapiclient import discovery\n",
    "    from tensorflow.python.lib.io import file_io\n",
    "    import os\n",
    "    \n",
    "    model_path = file_io.get_matching_files(os.path.join(model_path, 'export', 'export', '*'))[0]\n",
    "    api = discovery.build('ml', 'v1')\n",
    "    model_name, version_name = model_dot_version.split('.')\n",
    "    body = {'name': model_name}\n",
    "    parent = 'projects/%s' % gcp_project\n",
    "    try:\n",
    "        api.projects().models().create(body=body, parent=parent).execute()\n",
    "    except Exception as e:\n",
    "        # If the error is to create an already existing model. Ignore it.\n",
    "        print(str(e))\n",
    "        pass\n",
    "\n",
    "    import time\n",
    "\n",
    "    body = {\n",
    "        'name': version_name,\n",
    "        'deployment_uri': model_path,\n",
    "        'runtime_version': runtime\n",
    "    }\n",
    "\n",
    "    full_mode_name = 'projects/%s/models/%s' % (gcp_project, model_name)\n",
    "    response = api.projects().models().versions().create(body=body, parent=full_mode_name).execute()\n",
    "    \n",
    "    while True:\n",
    "        response = api.projects().operations().get(name=response['name']).execute()\n",
    "        if 'done' not in response or response['done'] is not True:\n",
    "            time.sleep(5)\n",
    "            print('still deploying...')\n",
    "        else:\n",
    "            if 'error' in response:\n",
    "                print(response['error'])\n",
    "            else:\n",
    "                print('Done.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function and make sure it works.\n",
    "path = 'gs://ml-pipeline-playground/sampledata/taxi/train'\n",
    "deploy_model(DEV_DEPLOYER_MODEL, path, PROJECT_NAME, '1.9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Pipeline Step With the Above Function(Note: run either of the two options below)\n",
    "#### Option One: Specify the dependency directly\n",
    "Now that we've tested the function locally, we want to build a component that can run as a step in the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-26 19:29:13:INFO:Build an image that is based on gcr.io/ml-pipeline-dogfood/pusherbase:dev and push the image to gcr.io/ml-pipeline-dogfood/pusher:dev\n",
      "2018-11-26 19:29:13:INFO:Checking path: gs://ngao-bugbash...\n",
      "2018-11-26 19:29:13:INFO:Generate entrypoint and serialization codes.\n",
      "2018-11-26 19:29:13:INFO:Generate build files.\n",
      "2018-11-26 19:29:13:INFO:Start a kaniko job for build.\n",
      "2018-11-26 19:29:18:INFO:5 seconds: waiting for job to complete\n",
      "2018-11-26 19:29:23:INFO:10 seconds: waiting for job to complete\n",
      "2018-11-26 19:29:28:INFO:15 seconds: waiting for job to complete\n",
      "2018-11-26 19:29:33:INFO:20 seconds: waiting for job to complete\n",
      "2018-11-26 19:29:38:INFO:25 seconds: waiting for job to complete\n",
      "2018-11-26 19:29:43:INFO:30 seconds: waiting for job to complete\n",
      "2018-11-26 19:29:48:INFO:35 seconds: waiting for job to complete\n",
      "2018-11-26 19:29:53:INFO:40 seconds: waiting for job to complete\n",
      "2018-11-26 19:29:58:INFO:45 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:03:INFO:50 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:08:INFO:55 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:13:INFO:60 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:18:INFO:65 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:23:INFO:70 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:28:INFO:75 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:33:INFO:80 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:38:INFO:85 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:43:INFO:90 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:48:INFO:95 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:53:INFO:100 seconds: waiting for job to complete\n",
      "2018-11-26 19:30:58:INFO:105 seconds: waiting for job to complete\n",
      "2018-11-26 19:31:03:INFO:110 seconds: waiting for job to complete\n",
      "2018-11-26 19:31:08:INFO:115 seconds: waiting for job to complete\n",
      "2018-11-26 19:31:13:INFO:120 seconds: waiting for job to complete\n",
      "2018-11-26 19:31:18:INFO:125 seconds: waiting for job to complete\n",
      "2018-11-26 19:31:24:INFO:130 seconds: waiting for job to complete\n",
      "2018-11-26 19:31:29:INFO:135 seconds: waiting for job to complete\n",
      "2018-11-26 19:31:34:INFO:140 seconds: waiting for job to complete\n",
      "2018-11-26 19:31:39:INFO:145 seconds: waiting for job to complete\n",
      "2018-11-26 19:31:39:INFO:Kaniko job complete.\n",
      "2018-11-26 19:31:39:INFO:Build component complete.\n"
     ]
    }
   ],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "# The return value \"DeployerOp\" represents a step that can be used directly in a pipeline function\n",
    "DeployerOp = compiler.build_python_component(\n",
    "    component_func=deploy_model,\n",
    "    staging_gcs_path=OUTPUT_DIR,\n",
    "    dependency=[kfp.compiler.VersionedDependency(name='google-api-python-client', version='1.7.0')],\n",
    "    base_image='tensorflow/tensorflow:1.12.0-py3',\n",
    "    target_image=TARGET_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option Two: build a base docker container image with both tensorflow and google api client packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Checking path: gs://bradley-playground...\n",
      "INFO:root:Generate build files.\n",
      "INFO:root:Start a kaniko job for build.\n",
      "INFO:root:5 seconds: waiting for job to complete\n",
      "INFO:root:10 seconds: waiting for job to complete\n",
      "INFO:root:15 seconds: waiting for job to complete\n",
      "INFO:root:20 seconds: waiting for job to complete\n",
      "INFO:root:25 seconds: waiting for job to complete\n",
      "INFO:root:30 seconds: waiting for job to complete\n",
      "INFO:root:35 seconds: waiting for job to complete\n",
      "INFO:root:40 seconds: waiting for job to complete\n",
      "INFO:root:45 seconds: waiting for job to complete\n",
      "INFO:root:50 seconds: waiting for job to complete\n",
      "INFO:root:55 seconds: waiting for job to complete\n",
      "INFO:root:60 seconds: waiting for job to complete\n",
      "INFO:root:65 seconds: waiting for job to complete\n",
      "INFO:root:70 seconds: waiting for job to complete\n",
      "INFO:root:75 seconds: waiting for job to complete\n",
      "INFO:root:80 seconds: waiting for job to complete\n",
      "INFO:root:85 seconds: waiting for job to complete\n",
      "INFO:root:90 seconds: waiting for job to complete\n",
      "INFO:root:95 seconds: waiting for job to complete\n",
      "INFO:root:100 seconds: waiting for job to complete\n",
      "INFO:root:105 seconds: waiting for job to complete\n",
      "INFO:root:110 seconds: waiting for job to complete\n",
      "INFO:root:115 seconds: waiting for job to complete\n",
      "INFO:root:Kaniko job complete.\n",
      "INFO:root:Build image complete.\n"
     ]
    }
   ],
   "source": [
    "%%docker {BASE_IMAGE} {OUTPUT_DIR}\n",
    "FROM tensorflow/tensorflow:1.10.0-py3\n",
    "RUN pip3 install google-api-python-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the base docker container image is built, we can build a \"target\" container image that is base_image plus the python function as entry point. The target container image can be used as a step in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Build an image that is based on gcr.io/bradley-playground/pusher:dev and push the image to gcr.io/bradley-playground/pusher:latest\n",
      "INFO:root:Checking path: gs://bradley-playground...\n",
      "INFO:root:Generate entrypoint and serialization codes.\n",
      "INFO:root:Generate build files.\n",
      "INFO:root:Start a kaniko job for build.\n",
      "INFO:root:5 seconds: waiting for job to complete\n",
      "INFO:root:10 seconds: waiting for job to complete\n",
      "INFO:root:15 seconds: waiting for job to complete\n",
      "INFO:root:20 seconds: waiting for job to complete\n",
      "INFO:root:25 seconds: waiting for job to complete\n",
      "INFO:root:30 seconds: waiting for job to complete\n",
      "INFO:root:35 seconds: waiting for job to complete\n",
      "INFO:root:40 seconds: waiting for job to complete\n",
      "INFO:root:45 seconds: waiting for job to complete\n",
      "INFO:root:50 seconds: waiting for job to complete\n",
      "INFO:root:55 seconds: waiting for job to complete\n",
      "INFO:root:60 seconds: waiting for job to complete\n",
      "INFO:root:65 seconds: waiting for job to complete\n",
      "INFO:root:70 seconds: waiting for job to complete\n",
      "INFO:root:75 seconds: waiting for job to complete\n",
      "INFO:root:80 seconds: waiting for job to complete\n",
      "INFO:root:85 seconds: waiting for job to complete\n",
      "INFO:root:90 seconds: waiting for job to complete\n",
      "INFO:root:95 seconds: waiting for job to complete\n",
      "INFO:root:100 seconds: waiting for job to complete\n",
      "INFO:root:105 seconds: waiting for job to complete\n",
      "INFO:root:110 seconds: waiting for job to complete\n",
      "INFO:root:115 seconds: waiting for job to complete\n",
      "INFO:root:120 seconds: waiting for job to complete\n",
      "INFO:root:125 seconds: waiting for job to complete\n",
      "INFO:root:130 seconds: waiting for job to complete\n",
      "INFO:root:135 seconds: waiting for job to complete\n",
      "INFO:root:140 seconds: waiting for job to complete\n",
      "INFO:root:145 seconds: waiting for job to complete\n",
      "INFO:root:150 seconds: waiting for job to complete\n",
      "INFO:root:Kaniko job complete.\n",
      "INFO:root:Build component complete.\n"
     ]
    }
   ],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "# The return value \"DeployerOp\" represents a step that can be used directly in a pipeline function\n",
    "DeployerOp = compiler.build_python_component(\n",
    "    component_func=deploy_model,\n",
    "    staging_gcs_path=OUTPUT_DIR,\n",
    "    target_image=TARGET_IMAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the pipeline with the new deployer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My New Pipeline. It's almost the same as the original one with the last step deployer replaced.\n",
    "@dsl.pipeline(\n",
    "  name='TFX Taxi Cab Classification Pipeline Example',\n",
    "  description='Example pipeline that does classification with model analysis based on a public BigQuery dataset.'\n",
    ")\n",
    "def my_taxi_cab_classification(\n",
    "    output,\n",
    "    project,\n",
    "    model,\n",
    "    column_names=dsl.PipelineParam(\n",
    "        name='column-names',\n",
    "        value='gs://ml-pipeline-playground/tfx/taxi-cab-classification/column-names.json'),\n",
    "    key_columns=dsl.PipelineParam(name='key-columns', value='trip_start_timestamp'),\n",
    "    train=dsl.PipelineParam(\n",
    "        name='train',\n",
    "        value=TRAIN_DATA),\n",
    "    evaluation=dsl.PipelineParam(\n",
    "        name='evaluation',\n",
    "        value=EVAL_DATA),\n",
    "    validation_mode=dsl.PipelineParam(name='validation-mode', value='local'),\n",
    "    preprocess_mode=dsl.PipelineParam(name='preprocess-mode', value='local'),\n",
    "    preprocess_module: dsl.PipelineParam=dsl.PipelineParam(\n",
    "        name='preprocess-module',\n",
    "        value='gs://ml-pipeline-playground/tfx/taxi-cab-classification/preprocessing.py'),\n",
    "    target=dsl.PipelineParam(name='target', value='tips'),\n",
    "    learning_rate=dsl.PipelineParam(name='learning-rate', value=0.1),\n",
    "    hidden_layer_size=dsl.PipelineParam(name='hidden-layer-size', value=HIDDEN_LAYER_SIZE),\n",
    "    steps=dsl.PipelineParam(name='steps', value=STEPS),\n",
    "    predict_mode=dsl.PipelineParam(name='predict-mode', value='local'),\n",
    "    analyze_mode=dsl.PipelineParam(name='analyze-mode', value='local'),\n",
    "    analyze_slice_column=dsl.PipelineParam(name='analyze-slice-column', value='trip_start_hour')):\n",
    "    \n",
    "    \n",
    "    validation_output = '%s/{{workflow.name}}/validation' % output\n",
    "    transform_output = '%s/{{workflow.name}}/transformed' % output\n",
    "    training_output = '%s/{{workflow.name}}/train' % output\n",
    "    analysis_output = '%s/{{workflow.name}}/analysis' % output\n",
    "    prediction_output = '%s/{{workflow.name}}/predict' % output\n",
    "\n",
    "    validation = dataflow_tf_data_validation_op(train, evaluation, column_names, key_columns, project, validation_mode, validation_output).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "\n",
    "    preprocess = dataflow_tf_transform_op(train, evaluation, validation.outputs['schema'], project, preprocess_mode, preprocess_module, transform_output).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    training = tf_train_op(preprocess.output, validation.outputs['schema'], learning_rate, hidden_layer_size, steps, target, preprocess_module, training_output).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    analysis = dataflow_tf_model_analyze_op(training.output, evaluation, validation.outputs['schema'], project, analyze_mode, analyze_slice_column, analysis_output).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    prediction = dataflow_tf_predict_op(evaluation, validation.outputs['schema'], target, training.output, predict_mode, project, prediction_output).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    \n",
    "    # The new deployer. Note that the DeployerOp interface is similar to the function \"deploy_model\".\n",
    "    deploy = DeployerOp(gcp_project=project, model_dot_version=model, runtime='1.9', model_path=training.output).apply(gcp.use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit a new job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/cb2d51d0-e2be-11e8-93d0-42010a800048\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compiler.Compiler().compile(my_taxi_cab_classification,  'my-tfx.tar.gz')\n",
    "\n",
    "run = client.run_pipeline(exp.id, 'my-tfx', 'my-tfx.tar.gz',\n",
    "                          params={'output': OUTPUT_DIR,\n",
    "                                  'project': PROJECT_NAME,\n",
    "                                  'model': PROD_DEPLOYER_MODEL})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
